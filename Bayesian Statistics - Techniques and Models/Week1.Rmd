---
title: 'Coursera Notes for Bayesian Statistics: Techniques and Models'
author: "Jordan Katz"
date: "June 1, 2020"
output: pdf_document
---


# Week 1
### Bayesian Modeling

data(s) $y$, parameter(s) $\theta$  
likelihood: $p(y | \theta)$  
prior: $p(\theta)$  
posterior: $p(\theta | y)$  
$$
p(\theta | y) = \frac{p(y | \theta) p(\theta)}{\int p(y|\theta)p(\theta) d\theta}
\propto p(y | \theta) p(\theta)
$$
Denominator also called "normalizing constant" since $\theta$ does not occur in the expression. If we do not use conjugate priors, or if the models are more complicated, then the posterior distribution may not have a "standard" or well-known form.

The Maximum Likelihood Estimator (MLE) $\hat{\theta}$ is the argmax$_\theta$ of the likelihood function. We can use $\log$ likelihood since the $\log$ function is monotonically increasing.

### Monte Carlo Estimation
Using simulation to determine some properties of a distribution, e.g. mean, variance, probability of an event, quantiles (which all use integration)  

$\qquad$ *Example*: Suppose we have $\theta \sim$ Ga$(a,b)$ and want to know $E[\theta]$
$$
E[\theta] = \int_{-\infty}^\infty \theta p(\theta) d\theta = \int_0^\infty \theta \frac{b^a}{\Gamma(a)} \theta^{a-1} e^{-b\theta} d\theta = \frac{a}{b}
$$

To verify with Monte Carlo, take samples $\theta_i^*$ for $i=1,\dots,m$ from the Gamma distribution. Estimate sample mean as
$$
\overline{\theta^*} = \frac{1}{m} \sum_{i=1}^m \theta_i^*
$$

Suppose we have some function $h(\theta)$ and we want $E[h(\theta)]$. Can estimate
$$
E[h(\theta)] = \int h(\theta) p(\theta) d\theta \approx \frac{1}{m} \sum_{i=1}^m h(\theta_i^*)
$$
In particular, if $h(\theta)$ is $I_A(\theta)$, i.e. the indicator function for some event $A$, then we can approximate probabilities as well: $Pr[\theta \in A]$.

Question: How good is this estimate from sampling?
By the Central Limit Theorem we know  
$$
\overline{\theta^*} \stackrel{.}{\sim} N \Big( E(\theta), \frac{Var(\theta)}{m} \Big)
$$
The variance of the estimate is given by
$$
\widehat{Var(\theta)} = \frac{1}{m} \sum_{i=1}^m (\theta_i^* - \overline{\theta^*})^2
$$
The standard error (SE) is given by
$$
\sqrt{ \frac{\widehat{Var(\theta)}}{m}}
$$
```{r}
set.seed(32)

m=10000
a=2
b=1/3

theta = rgamma(n=m, shape=a, rate=b)

hist(theta, freq=FALSE)
curve(dgamma(x, shape=a, rate=b), col="blue", add=TRUE)

mean(theta) # Estimated mean
a/b # True mean

var(theta) # Estimated variance
a/b^2 # True variance

ind = theta < 5
mean(ind) # Estimated Prob[theta < 5]
pgamma(q=5, shape=a, rate=b) # True Prob[theta < 5]

quantile(theta, probs=0.9) # Estimated quantile
qgamma(p=0.9, shape=a, rate=b) # True quantile

se = sd(theta) / sqrt(m) # Standard error of mean
mean(theta) - 2*se # Lower bound CI
mean(theta) + 2*se # Upper bound CI
```

As we can see, Monte Carlo does a pretty good job.  

$\qquad$ *Example*: Suppose we have
$$ y|\phi \sim Bin(10, \phi) $$
$$ \phi \sim Beta(2,2) $$
and we want to simulate from marginal distribution of $y$ (which can be difficult to do in general). Can do the following procedure:

1. Draw $\phi_i^* \sim Beta(2,2)$
2. Given $\phi_i^*$, draw $y_i^* \sim Bin(10, \phi_i^*)$

Results in a list of independent pairs $(y_i^*, \phi_i^*)$ drawn from the joint distribution. Discarding the $\phi_i^*$s effectively results in a sample from the marginal distribution of $y$.

```{r}
m = 1e5

phi = rbeta(m, shape1=2, shape2=2)
y = rbinom(m, size=10, prob=phi)

table(y) / m
plot(table(y) / m) # Estimated marginal distribution of y

mean(y) # Estimate mean of y
```









