---
title: 'Coursera Notes for Bayesian Statistics: Techniques and Models'
author: "Jordan Katz"
date: "June 2, 2020"
output: pdf_document
---

# Week 2
### Metropolis-Hastings

Allows us to sample from generic distribution (whose normalizing constant may not be known). To accomplish this, we effectively construct a Markov Chain whose stationary distribution is the target distribution.

Say we want to know $p(\theta)$ but we only know $g(\theta)$ where $p(\theta) \propto q(\theta)$.

$\qquad$*Algorithm:*

1. Select initial value $\theta_0$
2. for $i=1, \dots, m$ repeat:
    a. Draw candidate $\theta^* \sim q(\theta^* | \theta_{i-1})$
    b. Define $\alpha = \frac{g(\theta^*) / q(\theta^* | \theta_{i-1})}{g(\theta_{i-1}) / q(\theta_{i-1} | \theta^*)} = \frac{g(\theta^*)}{g(\theta_{i-1})} \frac{q(\theta_{i-1}|\theta^*)} {q(\theta^*|\theta_{i-1}))}$  
        i. if $\alpha \geq 1$:  
            accept $\theta^*$ and set $\theta_i \leftarrow \theta^*$
        ii. $0 < \alpha < 1$:  
            with prob $\alpha$: accept $\theta^*$ and set $\theta_i \leftarrow \theta^*$  
            with prob $1-\alpha$: reject $\theta^*$ and set $\theta_i \leftarrow \theta_{i-1}$

Where $q$ here is the candidate generating distribution which may or may not depend on $\theta_{i-1}$.

One choice is to make $q$ the same distribution regardless of the value $\theta_{i-1}$. If we take this option, we want $q(\theta)$ to be similar to $p(\theta)$ to best approximate it. A high acceptance rate is a good sign here but still may want $q$ to have a larger variance then $p$ to assure we are exploring the space well.

Another choice -- one which *does* depend on $\theta_{i-1}$ -- is to choose a distribution $q$ that is centered on $\theta_{i-1}$. In any symmetric case, we have the property $q(a|b) = q(b|a)$, so step 2 in the algorithm above reduces to
$$
\alpha = \frac{g(\theta^*)}{g(\theta_{i-1})}
$$

A common choice for such a distribution is $N(\theta_{i-1}, 1)$, or in order words, a Gaussian random walk: $\theta^* = \theta_{i-1} + N(0,1)$
In this particular case, we have
$$q(\theta^*|\theta_{i-1}) = \frac{1}{\sqrt{2 \pi}} \exp{[-0.5(\theta^* - \theta_{i-1})^2]} = q(\theta_{i-1}|\theta^*)$$
The "size" of the random walk step can affect acceptance (and thus convergence) rate. A high acceptance rate is not a good sign here. If random walk is taking too small of steps, it will accept candidate more often but will take a long time to fully explore the space. If it is taking too large of steps, many proposals will have low probabilities which leads to a low acceptance rate. This amounts to "wasted" samples. Ideally, a random walk sampler should have an acceptance rate between 23% and 50%.  


$\qquad$ *Example*: Suppose $y_i | \mu \stackrel{iid}{\sim} N(\mu, 1)$ for $i=1, \dots,n$ and $\mu \sim t(0,1,1)$. We want to sample from the posterior distribution $p(\mu | y_1, \dots, y_n)$, which we can analytically show is proportional to
$$g(\mu) :=  \frac{\exp[n(\overline{y} \mu - \mu^2/2) ]}{1 + \mu^2}$$

```{r}
# using log(g(x)) instead of g(x) for numerical stability
LOGg = function(mu, n, ybar) {
  n * (ybar * mu - mu^2 / 2) - log(1 + mu^2)
}
```

```{r}
metropolis_hastings = function(n, ybar, n_iter, mu_init, cand_sd) {
  # Random-Walk Metropolis-Hastings algorithm
  
  # initializations
  mu_out = numeric(n_iter)
  n_accept = 0
  
  # step 1
  mu_now = mu_init
  LOGg_now = LOGg(mu=mu_now, n=n, ybar=ybar)
  
  for (i in 1:n_iter) {
    # step 2a
    mu_cand = rnorm(1, mean=mu_now, sd=cand_sd) # draw candidate
    
    # step 2b
    LOGg_cand = LOGg(mu=mu_cand, n=n, ybar=ybar)
    LOGalpha = LOGg_cand - LOGg_now
    alpha = exp(LOGalpha)
    
    u = runif(1) # less than alpha with prob min(1, alpha)
    if (u < alpha) { # accept candidate
      n_accept = n_accept + 1
      mu_now = mu_cand
      LOGg_now = LOGg_cand
    }
    
    mu_out[i] = mu_now
  }
  
  list(mu=mu_out, accept_rate=n_accept/n_iter)
}
```

Problem set up:
```{r}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9) # data
ybar = mean(y) # sample mean
n = length(y)

hist(y, freq=FALSE, xlim=c(-1, 3)) # histogram of data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
```
```{r}
set.seed(43) # for reproducibility
library("coda") # traceplot --> helpful to determine convergence
```

Posterior sampling:
```{r}
post = metropolis_hastings(n=n, ybar=ybar, n_iter=1e3, mu_init=0, cand_sd=3)
str(post)
traceplot(as.mcmc(post$mu))
```

Step size too large (low acceptance rate). Let's try another.

```{r}
post = metropolis_hastings(n=n, ybar=ybar, n_iter=1e3, mu_init=0, cand_sd=0.05)
str(post)
traceplot(as.mcmc(post$mu))
```
Step size too small (high acceptance rate). Let's try another.

```{r}
post = metropolis_hastings(n=n, ybar=ybar, n_iter=1e3, mu_init=0, cand_sd=0.9)
str(post)
traceplot(as.mcmc(post$mu))
```
Looks good. Experimenting with different initial value:

```{r}
post = metropolis_hastings(n=n, ybar=ybar, n_iter=1e3, mu_init=30, cand_sd=0.9)
str(post)
traceplot(as.mcmc(post$mu))

post$mu_keep = post$mu[-c(1:100)] # discard the first 100 samples
str(post)

plot(density(post$mu_keep, adjust=2), main="", xlim=c(-1, 3), xlab=expression(mu))
# plot estimated posterior distribution

curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(ybar, 0, pch=19) # sample mean

curve(0.017*exp(LOGg(mu=x, n=n, ybar=ybar)), from=-1, to=3, add=TRUE, col="blue")
# approximation to the true posterior in blue
```

\newpage
### JAGS Software

1. Specify the model
2. Set up the model
3. Run the MCMC sampler
4. Post processing

(using same example as above)
```{r}
library("rjags")
set.seed(50)

# 1.
mod_string = "model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu, 1.0/sig2)
  }
  mu ~ dt(0.0, 1.0/1.0, 1)
  sig2 = 1.0
}"

# 2.
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9) # data
n = length(y)

data_jags = list(y=y, n=n)
params = c("mu")

inits = function() {
  inits = list("mu"=0.0)
}

mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)

# 3. 
update(mod, 500)
mod_sim = coda.samples(model=mod,
                       variable.names=params,
                       n.iter=1000)

# 4.
library("coda")
summary(mod_sim)
plot(mod_sim)
```


\newpage
### Gibbs Sampling
Question: What happens if we want a posterior distribution of multiple parameters?

Say we want to know $p(\theta, \phi | y)$ but we only know $g(\theta,\phi)$ where $p(\theta, \phi | y) \propto g(\theta,\phi)$.

By the chain rule of probability we have
$$
p(\theta | \phi, y) = \frac{ p(\theta, \phi | y) }{ p(\phi | y) }
                    \propto p(\theta, \phi | y)
                    \propto g(\theta,\phi)
$$
and by a similar analysis,
$$
p(\phi | \theta, y) \propto g(\theta, \phi)
$$


Main idea: iterate, taking turns drawing $\theta$ and $\phi$ one at a time, using the $g$ function.

$\qquad$*Algorithm:*

1. Initialize $\theta_0, \phi_0$
2. For $i=1, \dots, m$ repeat:
    a. Using $\phi_{i-1}$, draw $\theta_i \sim p(\theta | \phi_{i-1},y)$
    b. Using $\theta_i$, draw $\phi_i \sim p(\phi | \theta_i, y)$  
    results in a pair $(\theta_i, \phi_i)$
  
  
Note how this can be naturally extended to more than two parameters.

$\qquad$ *Example*: Suppose we have
\begin{align*}
y_i | \mu, \sigma^2 &\stackrel{iid}{\sim} N(\mu, \sigma^2), \qquad i=1,\dots,m \\
\mu &\sim N(\mu_0, \sigma_0) \\
\sigma^2 &\sim IG(\nu_0, \beta_0)
\end{align*}


\begin{align*}
p(\mu,\sigma^2 | y_1, \dots, y_n)
&\propto p(y_1, \dots, y_n | \mu, \sigma^2) p(\mu) p(\sigma^2) \\
&=
\end{align*}









